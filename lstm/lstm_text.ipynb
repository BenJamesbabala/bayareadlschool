{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "In this demo, you'll see a more practical application of RNNs/LSTMs as character-level language models. The emphasis will be more on parallelization and using RNNs with data from Fuel.\n",
    "\n",
    "To get started, we first need to download the training text, validation text and a file that contains a dictionary for mapping characters to integers. We also need to import quite a list of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from six.moves import cPickle as pkl\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano.tensor.nnet import categorical_crossentropy\n",
    "from theano import config\n",
    "config.floatX = 'float32'\n",
    "\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.transformers import Batch, Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "The code below shows an implementation of an LSTM network. Note that there are various different variations of the LSTM in use and this one doesn't include the so-called 'peephole connections'. We used a separate method for the dynamic update to make it easier to generate from the network later. The `index_dot` function doesn't safe much verbosity, but it clarifies that certain dot products have been replaced with indexing operations because this network will be applied to discrete data. Last but not least, note the addition of the `mask` argument which is used to ignore certain parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_weight(rng, ndim_in, ndim_out=None, sd=.005):\n",
    "    if ndim_out is None:\n",
    "        ndim_out = ndim_in\n",
    "    W = rng.randn(ndim_in, ndim_out) * sd\n",
    "    return numpy.asarray(W, dtype=config.floatX)\n",
    "\n",
    "\n",
    "def index_dot(indices, w):\n",
    "    return w[indices.flatten()]\n",
    "\n",
    "\n",
    "class LstmLayer:\n",
    "\n",
    "    def __init__(self, rng, input, mask, n_in, n_h):\n",
    "\n",
    "        # Init params\n",
    "        self.W_i = theano.shared(gauss_weight(rng, n_in, n_h), 'W_i', borrow=True)\n",
    "        self.W_f = theano.shared(gauss_weight(rng, n_in, n_h), 'W_f', borrow=True)\n",
    "        self.W_c = theano.shared(gauss_weight(rng, n_in, n_h), 'W_c', borrow=True)\n",
    "        self.W_o = theano.shared(gauss_weight(rng, n_in, n_h), 'W_o', borrow=True)\n",
    "\n",
    "        self.U_i = theano.shared(gauss_weight(rng, n_h), 'U_i', borrow=True)\n",
    "        self.U_f = theano.shared(gauss_weight(rng, n_h), 'U_f', borrow=True)\n",
    "        self.U_c = theano.shared(gauss_weight(rng, n_h), 'U_c', borrow=True)\n",
    "        self.U_o = theano.shared(gauss_weight(rng, n_h), 'U_o', borrow=True)\n",
    "\n",
    "        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n",
    "                                 'b_i', borrow=True)\n",
    "        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n",
    "                                 'b_f', borrow=True)\n",
    "        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n",
    "                                 'b_c', borrow=True)\n",
    "        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n",
    "                                 'b_o', borrow=True)\n",
    "\n",
    "        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n",
    "                       self.U_i, self.U_f, self.U_c, self.U_o,\n",
    "                       self.b_i, self.b_f, self.b_c, self.b_o]\n",
    "\n",
    "        outputs_info = [T.zeros((input.shape[1], n_h)),\n",
    "                        T.zeros((input.shape[1], n_h))]\n",
    "\n",
    "        rval, updates = theano.scan(self._step,\n",
    "                                    sequences=[mask, input],\n",
    "                                    outputs_info=outputs_info)\n",
    "\n",
    "        # self.output is in the format (length, batchsize, n_h)\n",
    "        self.output = rval[0]\n",
    "\n",
    "    def _step(self, m_, x_, h_, c_):\n",
    "\n",
    "        i_preact = (index_dot(x_, self.W_i) +\n",
    "                    T.dot(h_, self.U_i) + self.b_i)\n",
    "        i = T.nnet.sigmoid(i_preact)\n",
    "\n",
    "        f_preact = (index_dot(x_, self.W_f) +\n",
    "                    T.dot(h_, self.U_f) + self.b_f)\n",
    "        f = T.nnet.sigmoid(f_preact)\n",
    "\n",
    "        o_preact = (index_dot(x_, self.W_o) +\n",
    "                    T.dot(h_, self.U_o) + self.b_o)\n",
    "        o = T.nnet.sigmoid(o_preact)\n",
    "\n",
    "        c_preact = (index_dot(x_, self.W_c) +\n",
    "                    T.dot(h_, self.U_c) + self.b_c)\n",
    "        c = T.tanh(c_preact)\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n",
    "\n",
    "        h = o * T.tanh(c)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block contains some code that computes cross-entropy for masked sequences and a stripped down version of the logistic regression class from the deep learning tutorials which we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_categorical_crossentropy(prediction, targets, mask):\n",
    "    prediction_flat = prediction.reshape(((prediction.shape[0] *\n",
    "                                           prediction.shape[1]),\n",
    "                                          prediction.shape[2]), ndim=2)\n",
    "    targets_flat = targets.flatten()\n",
    "    mask_flat = mask.flatten()\n",
    "    ce = categorical_crossentropy(prediction_flat, targets_flat)\n",
    "    return T.sum(ce * mask_flat)\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "   \n",
    "    def __init__(self, rng, input, n_in, n_out):\n",
    "        \n",
    "        W = gauss_weight(rng, n_in, n_out)\n",
    "        self.W = theano.shared(value=numpy.asarray(W, dtype=theano.config.floatX),\n",
    "                               name='W', borrow=True)\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,),\n",
    "                                                 dtype=theano.config.floatX),\n",
    "                               name='b', borrow=True)\n",
    "\n",
    "        # compute vector of class-membership probabilities in symbolic form\n",
    "        energy = T.dot(input, self.W) + self.b\n",
    "        energy_exp = T.exp(energy - T.max(energy, axis=2, keepdims=True))\n",
    "        pmf = energy_exp / energy_exp.sum(axis=2, keepdims=True)\n",
    "        self.p_y_given_x = pmf\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Data\n",
    "The data in `traindata.txt` and `valdata.txt` is simply English text but formatted in such a way that every sentence is conveniently separated by the newline symbol. We'll use some of the functionality of fuel to perform the following preprocessing steps:\n",
    "* Convert everything to lowercase\n",
    "* Map characters to indices\n",
    "* Group the sentences into batches\n",
    "* Convert each batch in a matrix/tensor as long as the longest sequence with zeros padded to all the shorter sequences\n",
    "* Add a mask matrix that encodes the length of each sequence (a timestep at which the mask is 0 indicates that there is no data available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 40\n",
    "n_h = 50\n",
    "DICT_FILE = 'dictionary.pkl'\n",
    "TRAIN_FILE = 'traindata.txt'\n",
    "VAL_FILE = 'valdata.txt'\n",
    "\n",
    "# Load the datasets with Fuel\n",
    "dictionary = pkl.load(open(DICT_FILE, 'rb'))\n",
    "# add a symbol for unknown characters\n",
    "dictionary['~'] = len(dictionary)\n",
    "reverse_mapping = dict((j, i) for i, j in dictionary.items())\n",
    "n_chars = len(dictionary)\n",
    "\n",
    "train = TextFile(files=[TRAIN_FILE],\n",
    "                 dictionary=dictionary,\n",
    "                 unk_token='~',\n",
    "                 level='character',\n",
    "                 preprocess=str.lower,\n",
    "                 bos_token=None,\n",
    "                 eos_token=None)\n",
    "\n",
    "train_stream = DataStream.default_stream(train)\n",
    "\n",
    "# organize data in batches and pad shorter sequences with zeros\n",
    "train_stream = Batch(train_stream,\n",
    "                     iteration_scheme=ConstantScheme(batch_size))\n",
    "train_stream = Padding(train_stream)\n",
    "\n",
    "# idem dito for the validation text\n",
    "val = TextFile(files=[VAL_FILE],\n",
    "                 dictionary=dictionary,\n",
    "                 unk_token='~',\n",
    "                 level='character',\n",
    "                 preprocess=str.lower,\n",
    "                 bos_token=None,\n",
    "                 eos_token=None)\n",
    "\n",
    "val_stream = DataStream.default_stream(val)\n",
    "\n",
    "# organize data in batches and pad shorter sequences with zeros\n",
    "val_stream = Batch(val_stream,\n",
    "                     iteration_scheme=ConstantScheme(batch_size))\n",
    "val_stream = Padding(val_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theano Graph\n",
    "We'll now define the complete Theano graph for computing costs and gradients among other things. The cost will be the cross-entropy of the next character in the sequence and the network will try to predict it based on the previous characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the random number generator' seeds for consistency\n",
    "rng = numpy.random.RandomState(12345)\n",
    "\n",
    "x = T.lmatrix('x')\n",
    "mask = T.matrix('mask')\n",
    "\n",
    "# Construct an LSTM layer and a logistic regression layer\n",
    "recurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=n_chars, n_h=n_h)\n",
    "logreg_layer = LogisticRegression(rng=rng, input=recurrent_layer.output[:-1],\n",
    "                                  n_in=n_h, n_out=n_chars)\n",
    "\n",
    "# define a cost variable to optimize\n",
    "cost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n",
    "                                         x[1:],\n",
    "                                         mask[1:]) / batch_size\n",
    "\n",
    "# create a list of all model parameters to be fit by gradient descent\n",
    "params = logreg_layer.params + recurrent_layer.params\n",
    "\n",
    "# create a list of gradients for all model parameters\n",
    "grads = T.grad(cost, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile the function that updates the gradients. We also added a function that computes the cost without updating for monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "updates = [\n",
    "    (param_i, param_i - learning_rate * grad_i)\n",
    "    for param_i, grad_i in zip(params, grads)\n",
    "]\n",
    "\n",
    "update_model = theano.function([x, mask], cost, updates=updates)\n",
    "\n",
    "evaluate_model = theano.function([x, mask], cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sequences\n",
    "To see if the networks learn something useful (and to make results monitoring more entertaining), we'll also write some code to generate sequences. For this, we'll first compile a function that computes a single state update for the network to have more control over the values of each variable at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_t = T.iscalar()\n",
    "h_p = T.vector()\n",
    "c_p = T.vector()\n",
    "h_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\n",
    "energy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n",
    "\n",
    "energy_exp = T.exp(energy - T.max(energy, axis=1, keepdims=True))\n",
    "\n",
    "output = energy_exp / energy_exp.sum(axis=1, keepdims=True)\n",
    "single_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n",
    "\n",
    "def speak(single_step, prefix='the meaning of life is ', n_steps=450):\n",
    "    try:\n",
    "        h_p = numpy.zeros((n_h,), dtype=config.floatX)\n",
    "        c_p = numpy.zeros((n_h,), dtype=config.floatX)\n",
    "        sentence = prefix\n",
    "        for char in prefix:\n",
    "            x_t = dictionary[char]\n",
    "            prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n",
    "                                               c_p.flatten())\n",
    "        # Renormalize probability in float64\n",
    "        flat_prediction = prediction.flatten()\n",
    "        flat_pred_sum = flat_prediction.sum(dtype='float64')\n",
    "        if flat_pred_sum > 1:\n",
    "            flat_prediction = flat_prediction.astype('float64') / flat_pred_sum\n",
    "        sample = numpy.random.multinomial(1, flat_prediction)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            x_t = numpy.argmax(sample)\n",
    "            prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n",
    "                                               c_p.flatten())\n",
    "            # Renormalize probability in float64\n",
    "            flat_prediction = prediction.flatten()\n",
    "            flat_pred_sum = flat_prediction.sum(dtype='float64')\n",
    "            if flat_pred_sum > 1:\n",
    "                flat_prediction = flat_prediction.astype('float64') / flat_pred_sum\n",
    "            sample = numpy.random.multinomial(1, flat_prediction)\n",
    "\n",
    "            sentence += reverse_mapping[x_t]\n",
    "\n",
    "        return sentence\n",
    "    except ValueError as e:\n",
    "        print('Something went wrong during sentence generation: {}'.format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "\n",
      "LSTM: \"the meaning of life is uehhhpae s  dG n hm tsit²oewganrn, r iee anyas d zfeoyc!sc ghaY olt s  o ner8lw o  fihrmes ieraphg ywnE mg, t葱esa ie i ītf td m oo searhÉ, a ,oht f Cols uayeye ih et antw boon¡r uoi, rr hnrs , ee tc in, srlee lm ije sm e aer hy e eĂwuo wu io do s c ireno s lrt-c rnhrmo sady it w ee le yyhh !fneubeรw, se uoioueo Os my aeacghaŤr es ir.oet tR lt. astubo ber.Trd,cHiu vüat or cs r h1 i eoleufel tnuat r amu tei coes e wh ioachbiae t tyh c hp tovsi, ěl\"\n",
      "\n",
      "epoch: 0   minibatch: 40\n",
      "Average validation CE per sentence: 237.748261287\n",
      "\n",
      "LSTM: \"the meaning of life is et pee iis itiesrinc thoeK or.ior basreaettatvrf hede rhed oaëe sce an. mteuwssithncnsQeed sfu4d nawt cuaite whe l melatosadetolnīcsi, wy is&es ith w my cólt wok £e5liiy wendrausm pr tt.e che mr, toisegee fe nrdns ooorrt cni hye oie ttsÅ was , e. ynooucatunfacs se te atbps.an¡ote niwdisereverbitamtl rhc cac thofy *oCldicthec, nhr teevrinums theil cets toor g c p uteaurte to. theinrd invs rt as uuleers tr onst่gd ao.e dTrye, byoneitoe oè m bLee pl\"\n",
      "\n",
      "epoch: 0   minibatch: 80\n",
      "Average validation CE per sentence: 215.476670583\n",
      "\n",
      "LSTM: \"the meaning of life is orhisuss on) on.y.so., \"lant har ans eond yantheile de imt thint onal9 as( hhr me anile moufs pad fhhadh eอ in, sawe the rinntt 8is an. isunr6d fwanle thhe the a hef anato ig..es tom imil batu re wint whin fy thas the the uneinsessen-y, al lecind gon whorl 9ninp aegon dudoleusd orø poros th iny akU or cewet snasesolyut ghen onrieyhthe sht ne h^l'ie shac aue picorecses tham ohe el mepsiwe werice fi¡en a sarn r送oss tharin ce ind ans yathme nt bodes\"\n",
      "\n",
      "epoch: 0   minibatch: 120\n",
      "Average validation CE per sentence: 203.897479965\n",
      "\n",
      "LSTM: \"the meaning of life is the loy thie doougd orotheno wa s ancacy turlir ne f pomalpivecint hing., artibmhe che matt win'rere the in.eed ey wer.ing uriivels camiit ing wou gavimer we to uucat toucf a i au than votorophay od prmL e ma sy feane whe that reurbse alept agpicl thers font, castasxhbe toysM somy corhe ot thie routesy at ned as the basicel the Oe the therit tinumd ofom lin nandrdene hers atk intin, six thenkpss.anatremen dowind woues sland aniciguagme ine lesive\"\n",
      "\n",
      "epoch: 0   minibatch: 160\n",
      "Average validation CE per sentence: 194.586635212\n",
      "\n",
      "LSTM: \"the meaning of life is dutu 'f apras tho datl. o- car. theca tas that ons thek a the thast a blame fuks leout ong mhaa of paore antus soo pri=l i berato plim in a diks an he£is*ene oo tol. i poud to bemach ber betpune the folo ler wouncin hd wicthe samit a peous the nat a Thtren hasn che wog. arp ow pat to ares baid geniclitan ours a whay 送as bawe of mapl. olere a kaplary่an\" . rocop, thange iay one  e on tas siou sokl us i shes oá the e serpll bī hae tisaclg thissip-y\"\n",
      "\n",
      "epoch: 0   minibatch: 200\n",
      "Average validation CE per sentence: 190.865650053\n",
      "\n",
      "LSTM: \"the meaning of life is the whes heo pizpit he id lohe of the 1ly of dourdov thi.g in riton the wary the able in a lay seulit. re alelee noo ther that co theer naut. we to pade wa bnoédiot we we serend enly weur spit amtiche to do br ce the the thetreüer, whak llites tigips pabfuthins of fe uftetŤof wo the pilhe fouk, bige wengwer, inthe raig. whad we we yrisle to lytoet weringicp inkiy hick ant of repe the5obu, the'chind the co late seoribeade harr reunothentwin se min\"\n",
      "\n",
      "epoch: 0   minibatch: 240\n",
      "Average validation CE per sentence: 183.001058908\n",
      "\n",
      "LSTM: \"the meaning of life is is the thats..wecin de leltere coutcal hacg whot eto lot we ly bit tham -xatte ou wo0t allot*te aqe ,oor fonit. a loldis this dowkule my wo a low moakle now nee a this i wo whelere thy oom tont to douctr oun, prorer wat a the regoruts thoms achinl tose meat is ane o mexths audis se0 aenowith to de en pas homecit a mme the't is u s0aterty ,nit enhao teen hoflickigk, suw lotk of mat out i 7in eifce hat his' simalig of in gile that serang, insue wag\"\n",
      "\n",
      "epoch: 0   minibatch: 280\n",
      "Average validation CE per sentence: 180.368976695\n",
      "\n",
      "LSTM: \"the meaning of life is the a tons lot ham and go too goasther, or a do and crangerey or ind mrectld ephond cond, in nol sunturt of that meazesy coms, inatous thas to lepblat is is an cone to boxciscting out fidwedts'ss cos of aps, and oul veystris overe walss, it so pfror cillemind toucontel pit fuver thainn. to the the seres ling thes sathes hine.s thive dencon 1nersty thith a solvay andicke thak bolupast.r ins prcsenand bey? pictran in'ivan undues.rstis thig Čthet bu\"\n",
      "\n",
      "epoch: 0   minibatch: 320\n",
      "Average validation CE per sentence: 174.393929571\n",
      "\n",
      "LSTM: \"the meaning of life is this is thiss. foroad mep hat is the mave rutwand in yot frose the dneking no's mlatinat ald plewen ut i teranceang bacally dorliome-ogelb om theme ourtolyely are no ve hor dupe lreMemgre enon, is and that sing, on the yonnad in the lorg tite suclantitheris to be bugly kat botle ne'ars id fine, the boke0 pue' or is stiove isslise tomnes, ot stuves ixfersind thathing reg, 2byssely rastre.comer.'wh, qcarks ading cung wher ably fave los follads the \"\n",
      "\n",
      "epoch: 0   minibatch: 360\n",
      "Average validation CE per sentence: 170.669672827\n",
      "\n",
      "LSTM: \"the meaning of life is hane. are.ther saltel dourune try incenmional byays, wought yom it or'rededy buce\n",
      "ate isseemtrive its mime that oulliceser?ored wal -ecfuserend torule peray, and you henet a worl as imerdnsted prers. innanked hany, plot, deat ore-omenting con a the wald near a roully.y ereren- fistlidisnsoan. pive the culle tuilly protes.tting, ar in a flips2, sor-oute, alvirien asare wems siupl a courvind be prep-s. whigusp 2no\"s on doul't in the thes werñcpaok \"\n",
      "\n",
      "epoch: 0   minibatch: 400\n",
      "Average validation CE per sentence: 165.679857317\n",
      "\n",
      "LSTM: \"the meaning of life is the nodf of whe so thy is a dory we mar s orm'n in cove in exfar whe' prike, tho for steil del cal i. melinc it that whe  lish the be drean. the e me this aly it more por evele ith we the  im the tely id could ar trander at fur cord the that the sect cont and the gecs is the m^olsd the nots arren so to stor we the marplory re£ so all trat is of whe swer put yok we bE a prea dial the gost cun Qo nemlcition an ared for these tay dela had storb the \"\n",
      "\n",
      "epoch: 0   minibatch: 440\n",
      "Average validation CE per sentence: 170.729400073\n",
      "\n",
      "LSTM: \"the meaning of life is are in scas?. in astreving a ariwl we haprenters of thinksiad was horlal whilk s-wom hops ad a beople bigtecute ,irnating ar list 1f aspiagh enctangens to the the blot yor makion, ab themeriop plcined 2-pรe, mars besarys. was river bokitias id vers. taule viser nom, peanGly as grountvers plectiglavied. sory indersone on enga2salen, a'ly mo]t ous paralituis rowf id's that ecture herd xidh you where, signous to ktive hasn oub oul inVerxensim on tha\"\n",
      "\n",
      "epoch: 0   minibatch: 480\n",
      "Average validation CE per sentence: 160.242933204\n",
      "\n",
      "LSTM: \"the meaning of life is peaclule trogate with three works pack, uve be plees, exfpodat of whind forkble colrceallys as meallall sscam ene- hodige to will remenstioning the fon it stlice sayle qorban i'nl, e- haukses of lit\" apand wery.. difurt these's fre, recurs, y.od ifpeostieg., that'ng this, and than.sion be corluasting us peally anly i net. s-ponet's fastepicarsãnicg, thecend, ded'n wher inderedt's ngishing. freanimat i was mip epcebst me buk to nem on as whate poo\"\n",
      "\n",
      "epoch: 0   minibatch: 520\n",
      "Average validation CE per sentence: 157.124227072\n",
      "\n",
      "LSTM: \"the meaning of life is an the we'late aly wikis em.mation thy not of to lef tro'n froume\" thice oon in mace wiyv we cral and the is me wera scaule of to mome but -verecanai-late lemanabart a cone his hox e-time at me hayse of macul. prouge, chern that we gup the say dete this is leores of whree anseo treaters grers. no mes and she have we go spbuch. hy efle? and thousk lakt's the cancidion that strst a rice meres leg's a till evenien the busicm a mong byaye klowe. plan\"\n",
      "\n",
      "epoch: 0   minibatch: 560\n",
      "Average validation CE per sentence: 157.11385151\n",
      "\n",
      "LSTM: \"the meaning of life is omlaple??. xost, enowes. rehan it's xoment oret detcout a8texpion, ugpofia gain then is anddy that's gitn., wiine to have with abauple now your,, sces itflybery bl showrage, rost kleo, you to aif engentoly, stoge scaptan:, you, net ies, theing alling bedy exยeched how that than you ve den. if starts. el0ted, permli0ve pelicentink \"ihe. ecferfa  orsefthing apby, sodd buchurs youl. thry with us s1urs, baive, by ghe un sell scaining be loun ank i so\"\n",
      "\n",
      "epoch: 0   minibatch: 600\n",
      "Average validation CE per sentence: 153.375843582\n",
      "\n",
      "LSTM: \"the meaning of life is to hime't., of sopen sime cople. bicais, whicared we to had you vandew was could of to some and be itd in rig do the was lagk bemaang, bludo. und flucus twe culsi?, they, bioges verden no a  but no in meer the preopher and posa soun horse pendect. gom walk tas truckes an. a busce grow dove no on a pake of cay so the ond we canting and a twite is fabluw pacou fucoure she camking to in a sear asilu onn tyom if know to seand way a ferg to k's point \"\n",
      "\n",
      "epoch: 0   minibatch: 640\n",
      "Average validation CE per sentence: 152.911473837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c4bdd6620670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/blip/code/Theano/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/blip/code/Theano/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('epoch:', epoch)\n",
    "\n",
    "    for x_, mask_ in train_stream.get_epoch_iterator():\n",
    "        iteration += 1\n",
    "\n",
    "        cross_entropy = update_model(x_.T, mask_.T)\n",
    "\n",
    "\n",
    "        # Generate some text after each 20 minibatches\n",
    "        if iteration % 40 == 0:\n",
    "            sentence = speak(single_step, prefix='the meaning of life is ', n_steps=450)\n",
    "            print()\n",
    "            print('LSTM: \"' + sentence + '\"')\n",
    "            print()\n",
    "            print('epoch:', epoch, '  minibatch:', iteration)\n",
    "            val_scores = []\n",
    "            for x_val, mask_val in val_stream.get_epoch_iterator():\n",
    "                val_scores.append(evaluate_model(x_val.T, mask_val.T))\n",
    "            print('Average validation CE per sentence:', numpy.mean(val_scores))\n",
    "\n",
    "end_time = time.clock()\n",
    "print('Optimization complete.')\n",
    "print('The code ran for %.2fm' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can take a while before the text starts to look more reasonable but here are some things to experiment with:\n",
    "* Smarter optimization algorithms (or at least momentum)\n",
    "* Initializing the recurrent weights orthogonally\n",
    "* The sizes of the initial weights and biases (think about what the gates do)\n",
    "* Different sentence prefixes\n",
    "* Changing the temperature of the character distribution during generation. What happens when you generate deterministically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
